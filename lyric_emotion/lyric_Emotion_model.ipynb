{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17876,"status":"ok","timestamp":1697165551551,"user":{"displayName":"정지영","userId":"13111470828002352153"},"user_tz":-540},"id":"7UT0hq2JvvLj","outputId":"8c65aba1-10f2-4f4e-9955-568da2e44943"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51602,"status":"ok","timestamp":1697165603148,"user":{"displayName":"정지영","userId":"13111470828002352153"},"user_tz":-540},"id":"Dc6-X1bLSzzi","outputId":"4fe18883-69a4-41cb-d132-82b2442a95a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.5)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n","Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2023.7.22)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.1\n","    Uninstalling graphviz-0.20.1:\n","      Successfully uninstalled graphviz-0.20.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Collecting gluonnlp==0.8.0\n","  Downloading gluonnlp-0.8.0.tar.gz (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0) (1.23.5)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.8.0-py3-none-any.whl size=292696 sha256=411fcbcefa806714b34f17cb0006fe1adafbf5c015c592c1907153589f0a079d\n","  Stored in directory: /root/.cache/pip/wheels/2d/cc/dc/7ec84dced25f738b8be400101abb67e4b50c905090a51017e4\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.8.0\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Collecting transformers\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n","Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-axd0zcj8/kobert-tokenizer_eb4fc4a97f5f48c4a6d3121f8cb8ce2b\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-axd0zcj8/kobert-tokenizer_eb4fc4a97f5f48c4a6d3121f8cb8ce2b\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: kobert_tokenizer\n","  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=0a1ba3e773da0b3e7cb994c968cf0877f5b007545d97b64f98fe6d7b5b7db4ad\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-phdnvzfn/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n","Successfully built kobert_tokenizer\n","Installing collected packages: kobert_tokenizer\n","Successfully installed kobert_tokenizer-0.1\n"]}],"source":["!pip install mxnet\n","!pip install gluonnlp==0.8.0\n","!pip install tqdm pandas\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch>=1.8.1\n","\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MF4bzlt0S9l0","executionInfo":{"status":"ok","timestamp":1697165615468,"user_tz":-540,"elapsed":12324,"user":{"displayName":"정지영","userId":"13111470828002352153"}},"outputId":"53bec62e-5207-4e50-e7e8-416d8193374c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/mxnet/optimizer/optimizer.py:163: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n","  warnings.warn('WARNING: New optimizer %s.%s is overriding '\n"]}],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrzZOASNWQMS"},"outputs":[],"source":["# Torch GPU 설정\n","device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device = torch.device(device_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"dOAxXnf9TIZK","executionInfo":{"status":"ok","timestamp":1697165616426,"user_tz":-540,"elapsed":406,"user":{"displayName":"정지영","userId":"13111470828002352153"}},"outputId":"bd29dd6b-5bfd-40d4-838d-091746f007a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\ndata.loc[(data[\\'Emotion\\']==\\'공포\\'), \\'Emotion\\'] = 0\\ndata.loc[(data[\\'Emotion\\']==\\'놀람\\'), \\'Emotion\\'] = 1\\ndata.loc[(data[\\'Emotion\\']==\\'분노\\'), \\'Emotion\\'] = 2\\ndata.loc[(data[\\'Emotion\\']==\\'슬픔\\'), \\'Emotion\\'] = 3\\ndata.loc[(data[\\'Emotion\\']==\\'중립\\'), \\'Emotion\\'] = 4\\ndata.loc[(data[\\'Emotion\\']==\\'행복\\'), \\'Emotion\\'] = 5\\ndata.loc[(data[\\'Emotion\\']==\\'혐오\\'), \\'Emotion\\'] = 6\\n\\ndata.drop_duplicates(subset = [\\'Sentence\\'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\\ndata[\\'Sentence\\'] = data[\\'Sentence\\'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\\ndata[\\'Sentence\\'] = data[\\'Sentence\\'].str.replace(\\'^ +\\', \"\") # 공백은 empty 값으로 변경\\ndata[\\'Sentence\\'].replace(\\'\\', np.nan, inplace=True) # 공백은 Null 값으로 변경\\ndata = data.dropna(how=\\'any\\') # Null 값 제거\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["data  = pd.read_csv(\"/content/drive/MyDrive/추천하솜_2023/정지영/train_data.csv\")\n","\n","#train dataset의 아래 코드 전처리는 이미 되어있는 상태임! train dataset은 총 7만 4천여 문장으로 구성된 한국어 단발성 댓글/대화를 7가지 감정으로 분류함.\n","'''\n","\n","data.loc[(data['Emotion']=='공포'), 'Emotion'] = 0\n","data.loc[(data['Emotion']=='놀람'), 'Emotion'] = 1\n","data.loc[(data['Emotion']=='분노'), 'Emotion'] = 2\n","data.loc[(data['Emotion']=='슬픔'), 'Emotion'] = 3\n","data.loc[(data['Emotion']=='중립'), 'Emotion'] = 4\n","data.loc[(data['Emotion']=='행복'), 'Emotion'] = 5\n","data.loc[(data['Emotion']=='혐오'), 'Emotion'] = 6\n","\n","data.drop_duplicates(subset = ['Sentence'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n","data['Sentence'] = data['Sentence'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n","data['Sentence'] = data['Sentence'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n","data['Sentence'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","data = data.dropna(how='any') # Null 값 제거\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIkkipxDYaPi"},"outputs":[],"source":["data_list = []\n","for q, label in zip(data['Sentence'], data['Emotion']):\n","    data = []\n","    data.append(q)\n","    data.append(str(label))\n","\n","    data_list.append(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqquCCROTdkt"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","dataset_train, dataset_test = train_test_split(data_list, test_size = 0.2, shuffle = True, random_state = 2023)"]},{"cell_type":"markdown","metadata":{"id":"EfDaVoXEX5AJ"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B85X1yk7ViJC"},"outputs":[],"source":["class BERTSentenceTransform:\n","    \"\"\"BERT style data transformation.\n","\n","    Parameters\n","    ----------\n","    tokenizer : BERTTokenizer.\n","        Tokenizer for the sentences.\n","    max_seq_length : int.\n","        Maximum sequence length of the sentences.\n","    pad : bool, default True\n","        Whether to pad the sentences to maximum length.\n","    pair : bool, default True\n","        Whether to transform sentences or sentence pairs.\n","    \"\"\"\n","\n","    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n","        self._tokenizer = tokenizer\n","        self._max_seq_length = max_seq_length\n","        self._pad = pad\n","        self._pair = pair\n","        self._vocab = vocab\n","\n","    def __call__(self, line):\n","        \"\"\"Perform transformation for sequence pairs or single sequences.\n","\n","        The transformation is processed in the following steps:\n","        - tokenize the input sequences\n","        - insert [CLS], [SEP] as necessary\n","        - generate type ids to indicate whether a token belongs to the first\n","        sequence or the second sequence.\n","        - generate valid length\n","\n","        For sequence pairs, the input is a tuple of 2 strings:\n","        text_a, text_b.\n","\n","        Inputs:\n","            text_a: 'is this jacksonville ?'\n","            text_b: 'no it is not'\n","        Tokenization:\n","            text_a: 'is this jack ##son ##ville ?'\n","            text_b: 'no it is not .'\n","        Processed:\n","            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n","            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n","            valid_length: 14\n","\n","        For single sequences, the input is a tuple of single string:\n","        text_a.\n","\n","        Inputs:\n","            text_a: 'the dog is hairy .'\n","        Tokenization:\n","            text_a: 'the dog is hairy .'\n","        Processed:\n","            text_a: '[CLS] the dog is hairy . [SEP]'\n","            type_ids: 0     0   0   0  0     0 0\n","            valid_length: 7\n","\n","        Parameters\n","        ----------\n","        line: tuple of str\n","            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n","            (text_a, text_b). For single sequences, the input is a tuple of single\n","            string: (text_a,).\n","\n","        Returns\n","        -------\n","        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n","        np.array: valid length in 'int32', shape (batch_size,)\n","        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n","\n","        \"\"\"\n","\n","        # convert to unicode\n","        text_a = line[0]\n","        if self._pair:\n","            assert len(line) == 2\n","            text_b = line[1]\n","\n","        tokens_a = self._tokenizer.tokenize(text_a)\n","        tokens_b = None\n","\n","        if self._pair:\n","            tokens_b = self._tokenizer(text_b)\n","\n","        if tokens_b:\n","            # Modifies `tokens_a` and `tokens_b` in place so that the total\n","            # length is less than the specified length.\n","            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","            self._truncate_seq_pair(tokens_a, tokens_b,\n","                                    self._max_seq_length - 3)\n","        else:\n","            # Account for [CLS] and [SEP] with \"- 2\"\n","            if len(tokens_a) > self._max_seq_length - 2:\n","                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n","\n","        # The embedding vectors for `type=0` and `type=1` were learned during\n","        # pre-training and are added to the wordpiece embedding vector\n","        # (and position vector). This is not *strictly* necessary since\n","        # the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        #vocab = self._tokenizer.vocab\n","        vocab = self._vocab\n","        tokens = []\n","        tokens.append(vocab.cls_token)\n","        tokens.extend(tokens_a)\n","        tokens.append(vocab.sep_token)\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens.extend(tokens_b)\n","            tokens.append(vocab.sep_token)\n","            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n","\n","        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The valid length of sentences. Only real  tokens are attended to.\n","        valid_length = len(input_ids)\n","\n","        if self._pad:\n","            # Zero-pad up to the sequence length.\n","            padding_length = self._max_seq_length - valid_length\n","            # use padding tokens for the rest\n","            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n","            segment_ids.extend([0] * padding_length)\n","\n","        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n","            np.array(segment_ids, dtype='int32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6OvHPd_WT2iw"},"outputs":[],"source":["from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n","                 pad, pair):\n","        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n","        #transform = nlp.data.BERTSentenceTransform(\n","        #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-kGd0PMlT6uN"},"outputs":[],"source":["# 하이퍼 파라미터 설정\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167,"referenced_widgets":["d7a9cd25cdd8476c8bde24b9156678d2","6c9ace9dcbee43a98b027faf24f8bb73","54d6330e67ce40ec99fd041ded3a3730","cb6bafa54f564c26b8084db200919204","e1974c0bcc1b465594423621dcb2dbe0","2a6ae49b5aa24ddb819283a6f6b09ef1","d6bdbae861e24b62a17ced9d9b071a22","e3c2bf0c5a424a8db758fe0c9701054c","c31f8602260b4ef489fe702142904522","f941bcbd642348708c84ee852a32bffd","aa598f0c7e7747ba8372ddacd6c0e13c","0ca0eec5d63a4b10b3f8034116128ead","f37cb13131f245d2882c3bea054ed00c","a6dae4830da641f7b55ee5c5897e8445","f5bbf4db55854381b5e640f9b824efd4","49fa41ed355840b08d821de1b699ff0f","650cd893b0414b3999f4202f4f14a04d","468c6ebf9e62422589fb1fce015644e0","57671f6738374b8db02e4690786d0844","b514b26b347f4c83b0569d99f238aadc","4508e66bd15f4980a2b0c296a4e6a341","65b4a72b585545639783867bb3370317","5c5e3071ba0442f0b478feaff7f39042","12bbfef569a645a5b1b9449aab3ff2b2","eabb2e9de24345afbbe14a6bfaa110c9","29f968959c7b44319d7e08e4187f0b21","675917aa848c41bba2e8e542ccdc6410","d25c5b4ee854475dba78cd10c1787199","95d87a98b24b43cc95270c8c79f585ea","e45701585d61458aaff37bfaae647409","3c2357a19d674f578148f310a4ac5d06","e5029eb4d0ce4ad2b9ba144ac614e55a","c09a12fa3f50441d839f9644847337b8"]},"id":"AmEexD22u3yc","executionInfo":{"status":"ok","timestamp":1697165618565,"user_tz":-540,"elapsed":649,"user":{"displayName":"정지영","userId":"13111470828002352153"}},"outputId":"ff53c4e2-b8c7-4fd3-bfba-298a79b31f3a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7a9cd25cdd8476c8bde24b9156678d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)ve/main/spiece.model:   0%|          | 0.00/371k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ca0eec5d63a4b10b3f8034116128ead"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c5e3071ba0442f0b478feaff7f39042"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n","The class this function is called from is 'KoBERTTokenizer'.\n"]}],"source":["from kobert_tokenizer import KoBERTTokenizer\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-Urf5VJu6Hs"},"outputs":[],"source":["# kobert 공식 git에 있는 get_kobert_model 선언\n","def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n","    bertmodel = BertModel.from_pretrained(model_path)\n","    device = torch.device(ctx)\n","    bertmodel.to(device)\n","    bertmodel.eval()\n","    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file,\n","                                                         padding_token='[PAD]')\n","    return bertmodel, vocab_b_obj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrN9fEMMu9oE","executionInfo":{"status":"ok","timestamp":1697165624206,"user_tz":-540,"elapsed":5644,"user":{"displayName":"정지영","userId":"13111470828002352153"}},"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["209857b0452a457c8f2d51d97e248db4","a5481835dab1446398c7e678b133ed62","5b2fb2acf55941f3b5dd55f6f9129299","54fba2ebcec54103b5b1a05caffc53cf","fe813f6cf6ab4cf880a8740f08b05624","f494276c97a649f09e39a5e3dd302fa3","1016bede4c6e4e3fb8b85a5b3ae4220a","5555735acb1c474291111d144ab51e44","a7d3bdec8bde46689f22da115a607cf8","e367ed16ecf848eab938770a2fd671a7","0f803d32ea9b48d3a066170957225d47","99a08c107bd04b359fa7fd92a3fae988","99cb1d97dc244097946d1027016f80a0","bad06e32fa9e47c5b336a9cd99e8127a","e353ba366bad4508b25f628d949f10f2","1d07d74e42a74731a1a50746baf85793","f10db23e6416478cb49ab2460b241a32","64327b7f32bd4d91bd32d9a6d558f90e","89f7909e661943999e1a99bebfbf11be","e7b2956d450f45559190e3be017e920c","906ae798219b4114a70bc93aab2afb77","2505de739f644cb7a3fc2c1e0a76fdba"]},"outputId":"c8d00803-3270-42ee-ba84-3517f0474436"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/535 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"209857b0452a457c8f2d51d97e248db4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/369M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a08c107bd04b359fa7fd92a3fae988"}},"metadata":{}}],"source":["import gluonnlp as nlp\n","from transformers import BertModel\n","bertmodel, vocab = get_kobert_model('skt/kobert-base-v1',tokenizer.vocab_file)\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFYVgElyT8R6"},"outputs":[],"source":["data_train = BERTDataset(dataset_train, 0, 1, tokenizer, vocab, max_len, True, False)\n","data_test = BERTDataset(dataset_test, 0, 1, tokenizer, vocab, max_len, True, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7pKcuyqT-tY","executionInfo":{"status":"ok","timestamp":1697165639951,"user_tz":-540,"elapsed":20,"user":{"displayName":"정지영","userId":"13111470828002352153"}},"outputId":"6ca3f739-826e-4972-9aac-d09a86dcc03e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["# torch 형식의 dataset을 만들어 입력 데이터셋의 전처리 마무리\n","train_dataloader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, num_workers = 5)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, num_workers = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Am2b8T3W0nY"},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes = 7,   # 감정 클래스 수\n","                 dr_rate = None,\n","                 params = None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","\n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p = dr_rate)\n","\n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict = False)\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yi4_YaIrZnnc"},"outputs":[],"source":["model = BERTClassifier(bertmodel,  dr_rate = 0.5).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8kvhMtKZsKj","executionInfo":{"status":"ok","timestamp":1697165649329,"user_tz":-540,"elapsed":25,"user":{"displayName":"정지영","userId":"13111470828002352153"}},"outputId":"247e1f69-753f-4a68-fc46-7e6fe8f47668"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate)\n","loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 loss function\n","\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step, num_training_steps = t_total)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LOcpXUfcZuQR","executionInfo":{"status":"ok","timestamp":1697165649329,"user_tz":-540,"elapsed":21,"user":{"displayName":"정지영","userId":"13111470828002352153"}},"outputId":"17ae0631-33cf-490e-ba71-3678818658b7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7e437e0a4700>"]},"metadata":{},"execution_count":19}],"source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","\n","train_dataloader"]},{"cell_type":"markdown","metadata":{"id":"fXrILUR3vXLi"},"source":["모델 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZjph3N5FPtM"},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtvAeBm44FgF"},"outputs":[],"source":["import math\n","import time\n","start = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8h49lY_ZxOb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1bc8b23c-8b52-48dd-d7c3-2128e9968216","executionInfo":{"status":"ok","timestamp":1697168849996,"user_tz":-540,"elapsed":3200683,"user":{"displayName":"정지영","userId":"13111470828002352153"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 1/934 [00:04<1:07:45,  4.36s/it]"]},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 1 loss 2.0391812324523926 train acc 0.046875\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 201/934 [02:08<07:38,  1.60it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 201 loss 1.832529067993164 train acc 0.21805037313432835\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 401/934 [04:14<05:37,  1.58it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 401 loss 1.2196511030197144 train acc 0.3324501246882793\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 601/934 [06:20<03:29,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 601 loss 1.0746289491653442 train acc 0.4112936772046589\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 801/934 [08:26<01:23,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 801 loss 1.3334732055664062 train acc 0.46116182896379526\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 934/934 [09:50<00:00,  1.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1 train acc 0.4854604081225275\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/234 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","100%|██████████| 234/234 [00:50<00:00,  4.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1 test acc 0.6563318513923353\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/934 [00:00<14:57,  1.04it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 2 batch id 1 loss 0.52249675989151 train acc 0.859375\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 201/934 [02:07<07:43,  1.58it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 2 batch id 201 loss 0.9874444007873535 train acc 0.6467661691542289\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 401/934 [04:13<05:35,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 2 batch id 401 loss 0.9182578921318054 train acc 0.6618999376558603\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 601/934 [06:19<03:29,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 2 batch id 601 loss 0.7905257940292358 train acc 0.6754107737104825\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 801/934 [08:25<01:23,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 2 batch id 801 loss 1.0144965648651123 train acc 0.6851201622971286\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 934/934 [09:49<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 2 train acc 0.6910573485645846\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 234/234 [00:50<00:00,  4.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 2 test acc 0.6805792493796526\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/934 [00:00<13:12,  1.18it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 3 batch id 1 loss 0.3392429053783417 train acc 0.890625\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 201/934 [02:07<07:41,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 3 batch id 201 loss 0.7310171127319336 train acc 0.7221703980099502\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 401/934 [04:13<05:37,  1.58it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 3 batch id 401 loss 0.6717451810836792 train acc 0.7391677057356608\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 601/934 [06:19<03:30,  1.58it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 3 batch id 601 loss 0.5628403425216675 train acc 0.7522098585690515\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 801/934 [08:25<01:23,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 3 batch id 801 loss 0.651739239692688 train acc 0.76104088639201\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 934/934 [09:49<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 3 train acc 0.7651381541937357\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 234/234 [00:51<00:00,  4.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 3 test acc 0.687185518334712\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/934 [00:00<14:53,  1.04it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 4 batch id 1 loss 0.33714643120765686 train acc 0.859375\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 201/934 [02:07<07:42,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 4 batch id 201 loss 0.5675109624862671 train acc 0.792910447761194\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 401/934 [04:13<05:34,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 4 batch id 401 loss 0.4600749611854553 train acc 0.8071228179551122\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 601/934 [06:19<03:29,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 4 batch id 601 loss 0.36515578627586365 train acc 0.8184796173044925\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 801/934 [08:25<01:23,  1.60it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 4 batch id 801 loss 0.4173367917537689 train acc 0.826310861423221\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 934/934 [09:48<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 4 train acc 0.8295189044750119\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 234/234 [00:50<00:00,  4.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 4 test acc 0.6980114419630549\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/934 [00:00<14:48,  1.05it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 batch id 1 loss 0.23562073707580566 train acc 0.90625\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 201/934 [02:06<07:42,  1.58it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 batch id 201 loss 0.4616917669773102 train acc 0.8399409203980099\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 401/934 [04:13<05:35,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 batch id 401 loss 0.43801233172416687 train acc 0.850607855361596\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 601/934 [06:18<03:29,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 batch id 601 loss 0.24543103575706482 train acc 0.8605709234608985\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 801/934 [08:24<01:23,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 batch id 801 loss 0.4116922616958618 train acc 0.8649539637952559\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 934/934 [09:48<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 5 train acc 0.8663550135193989\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 234/234 [00:50<00:00,  4.64it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 test acc 0.6977400399779432\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["train_history = []\n","test_history = []\n","loss_history = []\n","\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","        # print(label.shape, out.shape)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","            train_history.append(train_acc / (batch_id+1))\n","            loss_history.append(loss.data.cpu().numpy())\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    # train_history.append(train_acc / (batch_id+1))\n","\n","    # .eval() : nn.Module에서 train time과 eval time에서 수행하는 다른 작업을 수행할 수 있도록 switching 하는 함수\n","    # 즉, model이 Dropout이나 BatNorm2d를 사용하는 경우, train 시에는 사용하지만 evaluation을 할 때에는 사용하지 않도록 설정해주는 함수\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length = valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n","    test_history.append(test_acc / (batch_id+1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYLlNAAs4UUZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697168849997,"user_tz":-540,"elapsed":25,"user":{"displayName":"정지영","userId":"13111470828002352153"}},"outputId":"e2695a8e-7398-4ed2-8f92-e6d74222ffad"},"outputs":[{"output_type":"stream","name":"stdout","text":["53.35\n"]}],"source":["end = time.time()\n","print(round(end - start)/60)"]},{"cell_type":"markdown","metadata":{"id":"gR4uvQ8MDJs_"},"source":["모델 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"WQBgAlBe4DjX","executionInfo":{"status":"ok","timestamp":1697168849998,"user_tz":-540,"elapsed":10,"user":{"displayName":"정지영","userId":"13111470828002352153"}},"outputId":"a89ae2d2-8221-4a63-e250-1a0b3ae4e4f4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/추천하솜_2023/kobert_model'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}],"source":["import os\n","os.chdir('/content/drive/MyDrive/추천하솜_2023/kobert_model')\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niqJdqgV7A_U"},"outputs":[],"source":["path = '/content/drive/MyDrive/추천하솜_2023/kobert_model/'\n","torch.save(model, path + 'emotion_model.pt')  # 전체 모델 저장\n","torch.save(model.state_dict(), 'emotion_model_state_dict.pt')  # 모델 객체의 state_dict 저장\n","torch.save({\n","    'model': model.state_dict(),\n","    'optimizer': optimizer.state_dict()\n","}, 'emotion_all.tar')  # 여러 가지 값 저장, 학습 중 진행 상황 저장을 위해 epoch, loss 값 등 일반 scalar값 저장 가능"]},{"cell_type":"markdown","metadata":{"id":"etlYDIrI6YOc"},"source":["모델 및 환경설정 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cu1mOQ07QAG"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QeT6KcOB7Jc1"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/추천하솜_2023/kobert_model/')\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAwYQiSg7SpH"},"outputs":[],"source":["!pip install mxnet\n","!pip install gluonnlp==0.8.0\n","!pip install tqdm pandas\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch>=1.8.1\n","\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kLKAAva7YiX"},"outputs":[],"source":["# Torch GPU 설정\n","device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device = torch.device(device_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xu7vxFuD7efl"},"outputs":[],"source":["class BERTSentenceTransform:\n","    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n","        self._tokenizer = tokenizer\n","        self._max_seq_length = max_seq_length\n","        self._pad = pad\n","        self._pair = pair\n","        self._vocab = vocab\n","\n","    def __call__(self, line):\n","        # convert to unicode\n","        text_a = line[0]\n","        if self._pair:\n","            assert len(line) == 2\n","            text_b = line[1]\n","\n","        tokens_a = self._tokenizer.tokenize(text_a)\n","        tokens_b = None\n","\n","        if self._pair:\n","            tokens_b = self._tokenizer(text_b)\n","\n","        if tokens_b:\n","            self._truncate_seq_pair(tokens_a, tokens_b,\n","                                    self._max_seq_length - 3)\n","        else:\n","            if len(tokens_a) > self._max_seq_length - 2:\n","                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n","\n","        vocab = self._vocab\n","        tokens = []\n","        tokens.append(vocab.cls_token)\n","        tokens.extend(tokens_a)\n","        tokens.append(vocab.sep_token)\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens.extend(tokens_b)\n","            tokens.append(vocab.sep_token)\n","            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n","\n","        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n","        valid_length = len(input_ids)\n","\n","        if self._pad:\n","            # Zero-pad up to the sequence length.\n","            padding_length = self._max_seq_length - valid_length\n","            # use padding tokens for the rest\n","            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n","            segment_ids.extend([0] * padding_length)\n","\n","        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n","            np.array(segment_ids, dtype='int32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VUXs4q48llX"},"outputs":[],"source":["from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n","                 pad, pair):\n","        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMnjeqer9h5-"},"outputs":[],"source":["# 하이퍼 파라미터 설정\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zo-BE-zU9eDC"},"outputs":[],"source":["from kobert_tokenizer import KoBERTTokenizer\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LF0nRC69nIn"},"outputs":[],"source":["# kobert 공식 git에 있는 get_kobert_model 선언\n","def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n","    bertmodel = BertModel.from_pretrained(model_path)\n","    device = torch.device(ctx)\n","    bertmodel.to(device)\n","    bertmodel.eval()\n","    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file,\n","                                                         padding_token='[PAD]')\n","    return bertmodel, vocab_b_obj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuUad2oU9ugY"},"outputs":[],"source":["import gluonnlp as nlp\n","from transformers import BertModel\n","bertmodel, vocab = get_kobert_model('skt/kobert-base-v1',tokenizer.vocab_file)\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxfA6Vgd95_2"},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes = 7,   # 감정 클래스 수\n","                 dr_rate = None,\n","                 params = None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","\n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p = dr_rate)\n","\n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict = False)\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTlm6gaa-XZf"},"outputs":[],"source":["## 학습 모델 로드\n","PATH = '/content/drive/MyDrive/추천하솜_2023/kobert_model/'\n","model = torch.load(PATH + 'emotion_model.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n","model.load_state_dict(torch.load(PATH + 'emotion_model_state_dict.pt'))  # state_dict를 불러 온 후, 모델에 저장\n","\n","checkpoint = torch.load('emotion_all.tar')   # dict 불러오기\n","model.load_state_dict(checkpoint['model'])\n","optimizer.load_state_dict(checkpoint['optimizer'])"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d7a9cd25cdd8476c8bde24b9156678d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c9ace9dcbee43a98b027faf24f8bb73","IPY_MODEL_54d6330e67ce40ec99fd041ded3a3730","IPY_MODEL_cb6bafa54f564c26b8084db200919204"],"layout":"IPY_MODEL_e1974c0bcc1b465594423621dcb2dbe0"}},"6c9ace9dcbee43a98b027faf24f8bb73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a6ae49b5aa24ddb819283a6f6b09ef1","placeholder":"​","style":"IPY_MODEL_d6bdbae861e24b62a17ced9d9b071a22","value":"Downloading (…)okenizer_config.json: 100%"}},"54d6330e67ce40ec99fd041ded3a3730":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3c2bf0c5a424a8db758fe0c9701054c","max":432,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c31f8602260b4ef489fe702142904522","value":432}},"cb6bafa54f564c26b8084db200919204":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f941bcbd642348708c84ee852a32bffd","placeholder":"​","style":"IPY_MODEL_aa598f0c7e7747ba8372ddacd6c0e13c","value":" 432/432 [00:00&lt;00:00, 10.1kB/s]"}},"e1974c0bcc1b465594423621dcb2dbe0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a6ae49b5aa24ddb819283a6f6b09ef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bdbae861e24b62a17ced9d9b071a22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c2bf0c5a424a8db758fe0c9701054c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c31f8602260b4ef489fe702142904522":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f941bcbd642348708c84ee852a32bffd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa598f0c7e7747ba8372ddacd6c0e13c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ca0eec5d63a4b10b3f8034116128ead":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f37cb13131f245d2882c3bea054ed00c","IPY_MODEL_a6dae4830da641f7b55ee5c5897e8445","IPY_MODEL_f5bbf4db55854381b5e640f9b824efd4"],"layout":"IPY_MODEL_49fa41ed355840b08d821de1b699ff0f"}},"f37cb13131f245d2882c3bea054ed00c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_650cd893b0414b3999f4202f4f14a04d","placeholder":"​","style":"IPY_MODEL_468c6ebf9e62422589fb1fce015644e0","value":"Downloading (…)ve/main/spiece.model: 100%"}},"a6dae4830da641f7b55ee5c5897e8445":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57671f6738374b8db02e4690786d0844","max":371427,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b514b26b347f4c83b0569d99f238aadc","value":371427}},"f5bbf4db55854381b5e640f9b824efd4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4508e66bd15f4980a2b0c296a4e6a341","placeholder":"​","style":"IPY_MODEL_65b4a72b585545639783867bb3370317","value":" 371k/371k [00:00&lt;00:00, 3.64MB/s]"}},"49fa41ed355840b08d821de1b699ff0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"650cd893b0414b3999f4202f4f14a04d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"468c6ebf9e62422589fb1fce015644e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57671f6738374b8db02e4690786d0844":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b514b26b347f4c83b0569d99f238aadc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4508e66bd15f4980a2b0c296a4e6a341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65b4a72b585545639783867bb3370317":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c5e3071ba0442f0b478feaff7f39042":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12bbfef569a645a5b1b9449aab3ff2b2","IPY_MODEL_eabb2e9de24345afbbe14a6bfaa110c9","IPY_MODEL_29f968959c7b44319d7e08e4187f0b21"],"layout":"IPY_MODEL_675917aa848c41bba2e8e542ccdc6410"}},"12bbfef569a645a5b1b9449aab3ff2b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d25c5b4ee854475dba78cd10c1787199","placeholder":"​","style":"IPY_MODEL_95d87a98b24b43cc95270c8c79f585ea","value":"Downloading (…)cial_tokens_map.json: 100%"}},"eabb2e9de24345afbbe14a6bfaa110c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e45701585d61458aaff37bfaae647409","max":244,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c2357a19d674f578148f310a4ac5d06","value":244}},"29f968959c7b44319d7e08e4187f0b21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5029eb4d0ce4ad2b9ba144ac614e55a","placeholder":"​","style":"IPY_MODEL_c09a12fa3f50441d839f9644847337b8","value":" 244/244 [00:00&lt;00:00, 12.0kB/s]"}},"675917aa848c41bba2e8e542ccdc6410":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d25c5b4ee854475dba78cd10c1787199":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95d87a98b24b43cc95270c8c79f585ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e45701585d61458aaff37bfaae647409":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c2357a19d674f578148f310a4ac5d06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5029eb4d0ce4ad2b9ba144ac614e55a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c09a12fa3f50441d839f9644847337b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"209857b0452a457c8f2d51d97e248db4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5481835dab1446398c7e678b133ed62","IPY_MODEL_5b2fb2acf55941f3b5dd55f6f9129299","IPY_MODEL_54fba2ebcec54103b5b1a05caffc53cf"],"layout":"IPY_MODEL_fe813f6cf6ab4cf880a8740f08b05624"}},"a5481835dab1446398c7e678b133ed62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f494276c97a649f09e39a5e3dd302fa3","placeholder":"​","style":"IPY_MODEL_1016bede4c6e4e3fb8b85a5b3ae4220a","value":"Downloading (…)lve/main/config.json: 100%"}},"5b2fb2acf55941f3b5dd55f6f9129299":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5555735acb1c474291111d144ab51e44","max":535,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7d3bdec8bde46689f22da115a607cf8","value":535}},"54fba2ebcec54103b5b1a05caffc53cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e367ed16ecf848eab938770a2fd671a7","placeholder":"​","style":"IPY_MODEL_0f803d32ea9b48d3a066170957225d47","value":" 535/535 [00:00&lt;00:00, 40.5kB/s]"}},"fe813f6cf6ab4cf880a8740f08b05624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f494276c97a649f09e39a5e3dd302fa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1016bede4c6e4e3fb8b85a5b3ae4220a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5555735acb1c474291111d144ab51e44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7d3bdec8bde46689f22da115a607cf8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e367ed16ecf848eab938770a2fd671a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f803d32ea9b48d3a066170957225d47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99a08c107bd04b359fa7fd92a3fae988":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99cb1d97dc244097946d1027016f80a0","IPY_MODEL_bad06e32fa9e47c5b336a9cd99e8127a","IPY_MODEL_e353ba366bad4508b25f628d949f10f2"],"layout":"IPY_MODEL_1d07d74e42a74731a1a50746baf85793"}},"99cb1d97dc244097946d1027016f80a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f10db23e6416478cb49ab2460b241a32","placeholder":"​","style":"IPY_MODEL_64327b7f32bd4d91bd32d9a6d558f90e","value":"Downloading pytorch_model.bin: 100%"}},"bad06e32fa9e47c5b336a9cd99e8127a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89f7909e661943999e1a99bebfbf11be","max":368792544,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7b2956d450f45559190e3be017e920c","value":368792544}},"e353ba366bad4508b25f628d949f10f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_906ae798219b4114a70bc93aab2afb77","placeholder":"​","style":"IPY_MODEL_2505de739f644cb7a3fc2c1e0a76fdba","value":" 369M/369M [00:02&lt;00:00, 145MB/s]"}},"1d07d74e42a74731a1a50746baf85793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f10db23e6416478cb49ab2460b241a32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64327b7f32bd4d91bd32d9a6d558f90e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89f7909e661943999e1a99bebfbf11be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7b2956d450f45559190e3be017e920c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"906ae798219b4114a70bc93aab2afb77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2505de739f644cb7a3fc2c1e0a76fdba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}